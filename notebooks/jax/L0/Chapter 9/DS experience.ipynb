{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kj/filesystem-disk-unix.c++:1703: warning: PWD environment variable doesn't match current directory; pwd = /home/teo/OpenMined/PySyft\n",
      "/home/teo/anaconda3/envs/PySyft/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… The installed version of syft==0.8.1b1 matches the requirement >=0.8b0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8-beta\")\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite Store Path:\n",
      "!open file:///tmp/7bca415d13ed1ec841f0d0aede098dbb.sqlite\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SyftClient - test-domain-1 <7bca415d13ed1ec841f0d0aede098dbb>: PythonConnection>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "guest_domain_client = node.client\n",
    "guest_domain_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "guest_domain_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "Asset: test_data\n",
       "Pointer Id: dd76b11202f84aa0a5ef109f2d147711\n",
       "Description: test data for MNIST\n",
       "Total Data Subjects: 1\n",
       "Shape: (10000, 28, 28)\n",
       "Contributors: 0\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "syft.service.dataset.dataset.Asset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect available data\n",
    "results = guest_domain_client.api.services.dataset.get_all()\n",
    "results[0].assets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "# ATTENTION: ALL LIBRARIES USED SHOULD BE DEFINED INSIDE THE FUNCTION CONTEXT!!!\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def training_loop(train_dataset, test_dataset):\n",
    "    import numpy as onp\n",
    "    import jax.numpy as np\n",
    "    from jax import grad, jit, vmap, value_and_grad\n",
    "    from jax import random\n",
    "    \n",
    "    from jax.scipy.special import logsumexp\n",
    "    from jax.experimental import optimizers\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    key = random.PRNGKey(1)\n",
    "\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_layer(params, x):\n",
    "        \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "        return ReLU(np.dot(params[0], x) + params[1])\n",
    "    \n",
    "    def initialize_mlp(sizes, key):\n",
    "        \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "        keys = random.split(key, len(sizes))\n",
    "        # Initialize a single layer with Gaussian weights -  helper function\n",
    "        def initialize_layer(m, n, key, scale=1e-2):\n",
    "            w_key, b_key = random.split(key)\n",
    "            return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "        return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "    layer_sizes = [784, 512, 512, 10]\n",
    "    # Return a list of tuples of layer weights\n",
    "    params = initialize_mlp(layer_sizes, key)\n",
    "    \n",
    "    def forward_pass(params, in_array):\n",
    "        \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "        activations = in_array\n",
    "\n",
    "        # Loop over the ReLU hidden layers\n",
    "        for w, b in params[:-1]:\n",
    "            activations = relu_layer([w, b], activations)\n",
    "\n",
    "        # Perform final trafo to logits\n",
    "        final_w, final_b = params[-1]\n",
    "        logits = np.dot(final_w, activations) + final_b\n",
    "        return logits - logsumexp(logits)\n",
    "\n",
    "    # Make a batched version of the `predict` function\n",
    "    batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)\n",
    "    # Defining an optimizer in Jax\n",
    "    step_size = 1e-3\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    num_epochs = 10\n",
    "    num_classes = 10\n",
    "\n",
    "\n",
    "    def one_hot(x, k, dtype=np.float32):\n",
    "        \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "        return np.array(x[:, None] == np.arange(k), dtype)\n",
    "\n",
    "    def loss(params, in_arrays, targets):\n",
    "        \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
    "        preds = batch_forward(params, in_arrays)\n",
    "        return -np.sum(preds * targets)\n",
    "\n",
    "    def accuracy(params, dataset):\n",
    "        \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
    "        acc_total = 0\n",
    "        for batch_idx, (data, target) in enumerate(dataset):\n",
    "            images = np.array(data).reshape(data.size(0), 28*28)\n",
    "            targets = one_hot(np.array(target), num_classes)\n",
    "\n",
    "            target_class = np.argmax(targets, axis=1)\n",
    "            predicted_class = np.argmax(batch_forward(params, images), axis=1)\n",
    "            acc_total += np.sum(predicted_class == target_class)\n",
    "        return acc_total/len()\n",
    "    \n",
    "    @jit\n",
    "    def update(params, x, y, opt_state):\n",
    "        \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "        value, grads = value_and_grad(loss)(params, x, y)\n",
    "        opt_state = opt_update(0, grads, opt_state)\n",
    "        return get_params(opt_state), opt_state, value\n",
    "    \n",
    "    def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "        \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "        # Initialize placeholder for loggin\n",
    "        log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "        # Get the initial set of parameters\n",
    "        params = get_params(opt_state)\n",
    "\n",
    "        # Get initial accuracy after random init\n",
    "        train_acc = accuracy(params, train_dataset)\n",
    "        test_acc = accuracy(params, test_dataset)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "\n",
    "        # Loop over the training epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            for data, target in train_dataset:\n",
    "                if net_type == \"MLP\":\n",
    "                    # Flatten the image into 784 vectors for the MLP\n",
    "                    x = np.array(data).reshape(data.size(0), 28*28)\n",
    "                elif net_type == \"CNN\":\n",
    "                    # No flattening of the input required for the CNN\n",
    "                    x = np.array(data)\n",
    "                y = one_hot(np.array(target), num_classes)\n",
    "                params, opt_state, loss = update(params, x, y, opt_state)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "            epoch_time = time.time() - start_time\n",
    "            train_acc = accuracy(params, train_dataset)\n",
    "            test_acc = accuracy(params, test_dataset)\n",
    "            log_acc_train.append(train_acc)\n",
    "            log_acc_test.append(test_acc)\n",
    "            print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                        train_acc, test_acc))\n",
    "\n",
    "        return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "\n",
    "    train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                            opt_state,\n",
    "                                                            net_type=\"MLP\")\n",
    "\n",
    "    # Plot the loss curve over time\n",
    "    from helpers import plot_mnist_performance\n",
    "    plot_mnist_performance(train_loss, train_log, test_log,\n",
    "                        \"MNIST MLP Performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySyft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
