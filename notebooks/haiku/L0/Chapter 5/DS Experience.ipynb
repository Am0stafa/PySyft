{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku Level 0 Data Scientist Experience - Chapter 5\n",
    "## Part 1 - New account registration and code execution requests\n",
    "\n",
    "Link to the original Haiku tutorial: https://dm-haiku.readthedocs.io/en/latest/notebooks/parameter_sharing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8-beta\")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "guest_domain_client = node.client\n",
    "guest_domain_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "guest_domain_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "# ATTENTION: ALL LIBRARIES USED SHOULD BE DEFINED INSIDE THE FUNCTION CONTEXT!!!\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def example():\n",
    "    #@title Imports and accessory functions\n",
    "    import chex\n",
    "    import functools\n",
    "    import haiku as hk\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    def parameter_shapes(params):\n",
    "        \"\"\"Make printing parameters a little more readable.\"\"\"\n",
    "        return jax.tree_util.tree_map(lambda p: p.shape, params)\n",
    "\n",
    "\n",
    "    def transform_and_print_shapes(fn, x_shape=(2, 3)):\n",
    "        \"\"\"Print name and shape of the parameters.\"\"\"\n",
    "        rng = jax.random.PRNGKey(42)\n",
    "        x = jnp.ones(x_shape)\n",
    "\n",
    "        transformed_fn = hk.transform(fn)\n",
    "        params = transformed_fn.init(rng, x)\n",
    "        print('\\nThe name and shape of the parameters are:')\n",
    "        print(parameter_shapes(params))\n",
    "\n",
    "    def assert_all_equal(params_1, params_2):\n",
    "        assert all(jax.tree_util.tree_leaves(\n",
    "            jax.tree_util.tree_map(lambda a, b: (a == b).all(), params_1, params_2)))\n",
    "        \n",
    "    \n",
    "    w_init = hk.initializers.TruncatedNormal(stddev=1)\n",
    "\n",
    "    class SimpleModule(hk.Module):\n",
    "        \"\"\"A simple module class with one variable.\"\"\"\n",
    "\n",
    "        def __init__(self, output_channels, name=None):\n",
    "            super().__init__(name)\n",
    "            assert isinstance(output_channels, int)\n",
    "            self._output_channels = output_channels\n",
    "\n",
    "        def __call__(self, x):\n",
    "            w_shape = (x.shape[-1], self._output_channels)\n",
    "            w = hk.get_parameter(\"w\", w_shape, x.dtype, init=w_init)\n",
    "            return jnp.dot(x, w)\n",
    "        \n",
    "    def f(x):\n",
    "        # This instance will be named `a_simple_module`.\n",
    "        simple = SimpleModule(output_channels=2)\n",
    "        simple_out = simple(x)  # implicitly calls module_install.__call__()\n",
    "        print(f'The name assigned to \"simple\" is: \"{simple.module_name}\".')\n",
    "        return simple_out\n",
    "\n",
    "    transform_and_print_shapes(f)\n",
    "    \n",
    "    def f(x):\n",
    "        # This instance will be named `a_simple_module`.\n",
    "        simple_one = SimpleModule(output_channels=2)\n",
    "        # This instance will be named `a_simple_module_1`.\n",
    "        simple_two = SimpleModule(output_channels=2)\n",
    "        first_out = simple_one(x)\n",
    "        second_out = simple_two(x)\n",
    "        print(f'The name assigned to \"simple_one\" is: \"{simple_one.module_name}\".')\n",
    "        print(f'The name assigned to \"simple_two\" is: \"{simple_two.module_name}\".')\n",
    "        return first_out + second_out\n",
    "\n",
    "    transform_and_print_shapes(f)\n",
    "    \n",
    "    def f(x):\n",
    "        # This instance will be named `a_simple_module`.\n",
    "        simple_one = SimpleModule(output_channels=2)\n",
    "        first_out = simple_one(x)\n",
    "        second_out = simple_one(x)  # share parameters w/ previous call\n",
    "        print(f'The name assigned to \"simple_one\" is: \"{simple_one.module_name}\".')\n",
    "        return first_out + second_out\n",
    "\n",
    "    transform_and_print_shapes(f)\n",
    "    \n",
    "    class NestedModule(hk.Module):\n",
    "        \"\"\"A module class with a nested module created in the constructor.\"\"\"\n",
    "        def __init__(self, output_channels, name=None):\n",
    "            super().__init__(name)\n",
    "            assert isinstance(output_channels, int)\n",
    "            self._output_channels = output_channels\n",
    "            self.inner_simple = SimpleModule(self._output_channels)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            w_shape = (x.shape[-1], self._output_channels)\n",
    "            # Another variable that is also called `w`.\n",
    "            w = hk.get_parameter(\"w\", w_shape, x.dtype, init=w_init)\n",
    "            return jnp.dot(x, w) + self.inner_simple(x)\n",
    "        \n",
    "    def f(x):\n",
    "        # This will be named `a_nested_module` and the SimpleModule instance created\n",
    "        # inside it will be named `a_nested_module/a_simple_module`.\n",
    "        nested = NestedModule(output_channels=2)\n",
    "        nested_out = nested(x)\n",
    "        print('The name assigned to outer module (i.e., \"nested\") is: '\n",
    "                f'\"{nested.module_name}\".')\n",
    "        print('The name assigned to the inner module (i.e., inside \"nested\") is: \"'\n",
    "                f'{nested.inner_simple.module_name}\".')\n",
    "        return nested_out\n",
    "\n",
    "    transform_and_print_shapes(f)\n",
    "\n",
    "    class TwiceNestedModule(hk.Module):\n",
    "        \"\"\"A module class with a nested module containing a nested module.\"\"\"\n",
    "\n",
    "        def __init__(self, output_channels, name=None):\n",
    "            super().__init__(name)\n",
    "            assert isinstance(output_channels, int)\n",
    "            self._output_channels = output_channels\n",
    "            self.inner_nested = NestedModule(self._output_channels)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            w_shape = (x.shape[-1], self._output_channels)\n",
    "            w = hk.get_parameter(\"w\", w_shape, x.dtype, init=w_init)\n",
    "            return jnp.dot(x, w) + self.inner_nested(x)\n",
    "        \n",
    "    def f(x):\n",
    "        \"\"\"Create the module instances and inspect their names.\"\"\"\n",
    "        # Instantiate a NestedModule instance. This will be named `a_nested_module`.\n",
    "        # The SimpleModule instance created inside it will be named\n",
    "        # a_nested_module/a_simple_module`.\n",
    "        outer = TwiceNestedModule(output_channels=2)\n",
    "        outer_out = outer(x)\n",
    "        print(f'The name assigned to the most outer class is: \"{outer.module_name}\".')\n",
    "        print('The name assigned to the module inside \"double_nested\" is: \"'\n",
    "                f'{outer.inner_nested.module_name}\".')\n",
    "        print('The name assigned to the module inside it is \"'\n",
    "                f'{outer.inner_nested.inner_simple.module_name}\".')\n",
    "        return outer_out\n",
    "\n",
    "    transform_and_print_shapes(f)\n",
    "\n",
    "    def f(x):\n",
    "        \"\"\"A SimpleModule followed by a Linear layer.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        linear = hk.Linear(40)\n",
    "        return linear(out)\n",
    "\n",
    "    def g(x):\n",
    "        \"\"\"A SimpleModule followed by an MLP.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        return module_instance(x) * 2  # twice\n",
    "\n",
    "    # Transform both functions, and print their respective parameter shapes.\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones((2, 3))\n",
    "    transformed_f = hk.transform(f)\n",
    "    params_f = transformed_f.init(rng, x)\n",
    "    transformed_g = hk.transform(g)\n",
    "    params_g = transformed_g.init(rng, x)\n",
    "    print('f parameters:', parameter_shapes(params_f))\n",
    "    print('g parameters:', parameter_shapes(params_g))\n",
    "\n",
    "    # Transform both functions at once with hk.multi_transform , and print the\n",
    "    # resulting merged parameter structure.\n",
    "\n",
    "    def multitransform_f_and_g():\n",
    "        def template(x):\n",
    "            return f(x), g(x)\n",
    "        return template, (f, g)\n",
    "    init, (f_apply, g_apply) = hk.multi_transform(multitransform_f_and_g)\n",
    "    merged_params = init(rng, x)\n",
    "\n",
    "    print('\\nThe name and shape of the multi-transform parameters are:\\n',\n",
    "        parameter_shapes(merged_params))\n",
    "    \n",
    "    def f(x):\n",
    "        \"\"\"Apply SimpleModule to x.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        return out\n",
    "\n",
    "    def g(x):\n",
    "        \"\"\"Like f, but double the output\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        return out * 2\n",
    "\n",
    "    # Transform both functions, and print the parameter shapes.\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones((2, 3))\n",
    "\n",
    "    transformed_f = hk.transform(f)\n",
    "    params_f = transformed_f.init(rng, x)\n",
    "    transformed_g = hk.transform(g)\n",
    "    params_g = transformed_g.init(rng, x)\n",
    "\n",
    "    print('f parameters:', parameter_shapes(params_f))\n",
    "    print('g parameters:', parameter_shapes(params_g))\n",
    "    \n",
    "    def f(x):\n",
    "        \"\"\"A SimpleModule followed by a Linear layer.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        linear = hk.Linear(40)\n",
    "        return linear(out)\n",
    "\n",
    "    def g(x):\n",
    "        \"\"\"A SimpleModule followed by an MLP.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        linear = hk.nets.MLP((10, 40))\n",
    "        return linear(out)  \n",
    "\n",
    "    # Transform both functions, and print the parameter shapes.\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones((2, 3))\n",
    "\n",
    "    transformed_f = hk.transform(f)\n",
    "    params_f = transformed_f.init(rng, x)\n",
    "    transformed_g = hk.transform(g)\n",
    "    params_g = transformed_g.init(rng, x)\n",
    "\n",
    "    print('\\nThe name and shape of the f parameters are:\\n',\n",
    "        parameter_shapes(params_f))\n",
    "    print('\\nThe name and shape of the g parameters are:\\n',\n",
    "        parameter_shapes(params_g))\n",
    "    \n",
    "    merged_params = hk.data_structures.merge(params_f, params_g)\n",
    "    print('\\nThe name and shape of the shared parameters are:\\n',\n",
    "        parameter_shapes(merged_params))\n",
    "    \n",
    "    f_out = transformed_f.apply(merged_params, rng, x)\n",
    "    g_out = transformed_g.apply(merged_params, rng, x)\n",
    "\n",
    "    print('f_out mean:', f_out.mean())\n",
    "    print('g_out mean:', g_out.mean())\n",
    "    \n",
    "    def f(x):\n",
    "        \"\"\"A SimpleModule followed by two Linear layers.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        mlp = hk.nets.MLP((10, 5))\n",
    "        out = mlp(out)\n",
    "        last_linear = hk.Linear(4)\n",
    "        return last_linear(out)\n",
    "\n",
    "    def g(x):\n",
    "        \"\"\"Same as f, with a bigger final layer.\"\"\"\n",
    "        module_instance = SimpleModule(output_channels=2)\n",
    "        out = module_instance(x)\n",
    "        mlp = hk.nets.MLP((10, 5))\n",
    "        out = mlp(out)\n",
    "        last_linear = hk.Linear(20)  # another Linear, but bigger\n",
    "        return last_linear(out)\n",
    "\n",
    "    # Transform both functions, and print the parameter shapes.\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones((2, 3))\n",
    "\n",
    "    transformed_f = hk.transform(f)\n",
    "    params_f = transformed_f.init(rng, x)\n",
    "    transformed_g = hk.transform(g)\n",
    "    params_g = transformed_g.init(rng, x)\n",
    "\n",
    "    print('\\nThe name and shape of the f parameters are:\\n',\n",
    "        parameter_shapes(params_f))\n",
    "    print('\\nThe name and shape of the g parameters are:\\n',\n",
    "        parameter_shapes(params_g))\n",
    "    \n",
    "    merged_params = hk.data_structures.merge(params_f, params_g)\n",
    "    print('\\nThe name and shape of the merged parameters are:\\n',\n",
    "        parameter_shapes(merged_params))\n",
    "\n",
    "    try: \n",
    "        f_out = transformed_f.apply(merged_params, rng, x)  # fails\n",
    "        # ValueError: 'linear/w' with retrieved shape (5, 20) does not match shape=[5, 4] dtype=dtype('float32')\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        module_instance = SimpleModule(output_channels=2)  # this fails\n",
    "        # ValueError: All `hk.Module`s must be initialized inside an `hk.transform`.\n",
    "        mlp = hk.nets.MLP((10, 5))\n",
    "        def f(x):\n",
    "            \"\"\"A SimpleModule followed by a Linear layer.\"\"\"\n",
    "            out = module_instance(x)\n",
    "            out = mlp(out)\n",
    "            linear = hk.Linear(4)\n",
    "            return linear(out)\n",
    "\n",
    "        def g(x):\n",
    "            \"\"\"A SimpleModule followed by a bigger Linear layer.\"\"\"\n",
    "            out = module_instance(x)\n",
    "            out = mlp(out)\n",
    "            linear = hk.Linear(20)  # another Linear, but bigger\n",
    "            return linear(out)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    class CachedModule():\n",
    "\n",
    "        def __call__(self, *inputs):\n",
    "            # Create the instances if are not in the cache.\n",
    "            if not hasattr(self, 'cached_simple_module'):\n",
    "                self.cached_simple_module = SimpleModule(output_channels=2)\n",
    "            if not hasattr(self, 'cached_mlp'):\n",
    "                self.cached_mlp = hk.nets.MLP((10, 5))\n",
    "\n",
    "            # Apply the cached instances.\n",
    "            out = self.cached_simple_module(*inputs)\n",
    "            out = self.cached_mlp(out)\n",
    "            return out\n",
    "\n",
    "\n",
    "    def f(x):\n",
    "        \"\"\"A SimpleModule followed by a Linear layer.\"\"\"\n",
    "        shared_preprocessing = CachedModule()\n",
    "        out = shared_preprocessing(x)\n",
    "        linear = hk.Linear(4)\n",
    "        return linear(out)\n",
    "\n",
    "    def g(x):\n",
    "        \"\"\"A SimpleModule followed by a bigger Linear layer.\"\"\"\n",
    "        shared_preprocessing = CachedModule()\n",
    "        out = shared_preprocessing(x)\n",
    "        linear = hk.Linear(20)  # another Linear, but bigger\n",
    "        return linear(out)\n",
    "\n",
    "\n",
    "    # Transform both functions, and print the parameter shapes.\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones((2, 3))\n",
    "\n",
    "    transformed_f = hk.transform(f)\n",
    "    params_f = transformed_f.init(rng, x)\n",
    "    transformed_g = hk.transform(g)\n",
    "    params_g = transformed_g.init(rng, x)\n",
    "\n",
    "    print('\\nThe name and shape of the f parameters are:\\n',\n",
    "        parameter_shapes(params_f))\n",
    "    print('\\nThe name and shape of the g parameters are:\\n',\n",
    "        parameter_shapes(params_g))\n",
    "\n",
    "    # Verify that the simple module parameters are shared.\n",
    "    assert_all_equal(params_f['mlp/~/linear_0'],\n",
    "                    params_g['mlp/~/linear_0'])\n",
    "    assert_all_equal(params_f['mlp/~/linear_1'],\n",
    "                    params_g['mlp/~/linear_1'])\n",
    "    print('\\nThe MLP parameters are shared!')    \n",
    "    \n",
    "    def share_parameters():\n",
    "        def decorator(fn):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                if wrapper.instance is None:\n",
    "                    wrapper.instance = hk.to_module(fn)()\n",
    "                return wrapper.instance(*args, **kwargs)\n",
    "            wrapper.instance = None\n",
    "            return functools.wraps(fn)(wrapper)\n",
    "        return decorator\n",
    "\n",
    "\n",
    "    class Wrapper():\n",
    "\n",
    "        @share_parameters()\n",
    "        def shared_preprocessing(self, x):\n",
    "            simple_module = SimpleModule(output_channels=2)\n",
    "            out = simple_module(x)\n",
    "            mlp = hk.nets.MLP((10, 5))\n",
    "            return mlp(out)\n",
    "\n",
    "        def f(self, x):\n",
    "            \"\"\"A SimpleModule followed by a Linear layer.\"\"\"\n",
    "            out = self.shared_preprocessing(x)\n",
    "            linear = hk.Linear(4)\n",
    "            return linear(out)\n",
    "\n",
    "        def g(self, x):\n",
    "            \"\"\"A SimpleModule followed by a bigger Linear layer.\"\"\"\n",
    "            out = self.shared_preprocessing(x)\n",
    "            linear = hk.Linear(20)  # another Linear, but bigger\n",
    "            return linear(out)\n",
    "\n",
    "    # Transform both functions, and print the parameter shapes.\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones((2, 3))\n",
    "\n",
    "    wrapper = Wrapper()\n",
    "    transformed_f = hk.transform(wrapper.f)\n",
    "    params_f = transformed_f.init(rng, x)\n",
    "    transformed_g = hk.transform(wrapper.f)\n",
    "    params_g = transformed_g.init(rng, x)\n",
    "\n",
    "    print('\\nThe name and shape of the f parameters are:\\n',\n",
    "        parameter_shapes(params_f))\n",
    "    print('\\nThe name and shape of the g parameters are:\\n',\n",
    "        parameter_shapes(params_g))\n",
    "\n",
    "    # Verify that the simple module parameters are shared.\n",
    "    assert_all_equal(params_f['shared_preprocessing/mlp/~/linear_0'],\n",
    "                    params_g['shared_preprocessing/mlp/~/linear_0'])\n",
    "    assert_all_equal(params_f['shared_preprocessing/mlp/~/linear_1'],\n",
    "                    params_g['shared_preprocessing/mlp/~/linear_1'])\n",
    "    print('\\nThe MLP parameters are shared!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function locally \n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the function for code execution\n",
    "guest_domain_client.api.services.code.request_code_execution(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_domain_client.api.services.code.example()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the Data Owner Notebook for Part 2!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Downloading the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_domain_client._api = None\n",
    "_ = guest_domain_client.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = guest_domain_client.api.services.code.example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.get_stderr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.get_stdout())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySyft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
