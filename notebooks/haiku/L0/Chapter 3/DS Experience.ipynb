{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku Level 0 Data Scientist Experience - Chapter 3\n",
    "## Part 2 - New account registration and code execution requests\n",
    "\n",
    "Link to the original Haiku tutorial: https://dm-haiku.readthedocs.io/en/latest/notebooks/build_your_own_haiku.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8-beta\")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "guest_domain_client = node.client\n",
    "guest_domain_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "guest_domain_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "# ATTENTION: ALL LIBRARIES USED SHOULD BE DEFINED INSIDE THE FUNCTION CONTEXT!!!\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def example(ds_train, ds_test):\n",
    "    import haiku as hk\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    from jax.experimental import jax2tf\n",
    "    import sonnet as snt\n",
    "    import tensorflow as tf\n",
    "    import tree\n",
    "    \n",
    "    def f(x):\n",
    "        net = hk.nets.MLP([300, 100, 10])\n",
    "        return net(x)\n",
    "\n",
    "    f = hk.transform(f)\n",
    "\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    x = jnp.ones([1, 28 * 28 * 1])\n",
    "    params = f.init(rng, x)\n",
    "\n",
    "    def create_variable(path, value):\n",
    "        name = '/'.join(map(str, path)).replace('~', '_')\n",
    "        return tf.Variable(value, name=name)\n",
    "\n",
    "    class JaxModule(snt.Module):\n",
    "        def __init__(self, params, apply_fn, name=None):\n",
    "            super().__init__(name=name)\n",
    "            self._params = tree.map_structure_with_path(create_variable, params)\n",
    "            self._apply = jax2tf.convert(lambda p, x: apply_fn(p, None, x))\n",
    "            self._apply = tf.autograph.experimental.do_not_convert(self._apply)\n",
    "\n",
    "        def __call__(self, inputs):\n",
    "            return self._apply(self._params, inputs)\n",
    "\n",
    "    net = JaxModule(params, f.apply)\n",
    "    [v.name for v in net.trainable_variables]\n",
    "\n",
    "    def normalize_img(image, label):\n",
    "        \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "        image = tf.cast(image, tf.float32) / 255.\n",
    "        return image, label\n",
    "\n",
    "    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(60000)\n",
    "    ds_train = ds_train.batch(100)\n",
    "    ds_train = ds_train.repeat()\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_test = ds_test.batch(100)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    net = JaxModule(params, f.apply)\n",
    "    opt = snt.optimizers.Adam(1e-3)\n",
    "\n",
    "    @tf.function(experimental_compile=True, autograph=False)\n",
    "    def train_step(images, labels):\n",
    "        \"\"\"Performs one optimizer step on a single mini-batch.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            images = snt.flatten(images)\n",
    "            logits = net(images)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                labels=labels)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            params = tape.watched_variables()\n",
    "            loss += 1e-4 * sum(map(tf.nn.l2_loss, params))\n",
    "\n",
    "        grads = tape.gradient(loss, params)\n",
    "        opt.apply(grads, params)\n",
    "        return loss\n",
    "\n",
    "    for step, (images, labels) in enumerate(ds_train.take(6001)):\n",
    "        loss = train_step(images, labels)\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step}: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function locally \n",
    "first_example()\n",
    "stateful_inference_example()\n",
    "haiku_nets_example()\n",
    "hk_next_rng_key_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the function for code execution\n",
    "guest_domain_client.api.services.code.request_code_execution(first_example)\n",
    "guest_domain_client.api.services.code.request_code_execution(stateful_inference_example)\n",
    "guest_domain_client.api.services.code.request_code_execution(haiku_nets_example)\n",
    "guest_domain_client.api.services.code.request_code_execution(hk_next_rng_key_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_domain_client.api.services.code.first_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the Data Owner Notebook for Part 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Downloading the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_domain_client._api = None\n",
    "_ = guest_domain_client.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = guest_domain_client.api.services.code.haiku_nets_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.get_stderr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySyft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
