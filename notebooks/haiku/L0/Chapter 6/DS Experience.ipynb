{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku Level 0 Data Scientist Experience - Chapter 1\n",
    "## Part 2 - New account registration and code execution requests\n",
    "\n",
    "Link to the original Haiku tutorial: https://theaisummer.com/jax-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8-beta\")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "guest_domain_client = node.client\n",
    "guest_domain_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "guest_domain_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "# ATTENTION: ALL LIBRARIES USED SHOULD BE DEFINED INSIDE THE FUNCTION CONTEXT!!!\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def example():\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import haiku as hk\n",
    "    import numpy as np\n",
    "    from typing import Optional\n",
    "    from typing import Mapping\n",
    "    from typing import Any\n",
    "    import functools\n",
    "    import optax\n",
    "    import \n",
    "    \n",
    "    class SelfAttention(hk.MultiHeadAttention):\n",
    "        \"\"\"Self attention with a causal mask applied.\"\"\"\n",
    "\n",
    "        def __call__(\n",
    "                self,\n",
    "                query: jnp.ndarray,\n",
    "                key: Optional[jnp.ndarray] = None,\n",
    "                value: Optional[jnp.ndarray] = None,\n",
    "                mask: Optional[jnp.ndarray] = None,\n",
    "        ) -> jnp.ndarray:\n",
    "            key = key if key is not None else query\n",
    "            value = value if value is not None else query\n",
    "\n",
    "            seq_len = query.shape[1]\n",
    "            causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "            mask = mask * causal_mask if mask is not None else causal_mask\n",
    "\n",
    "            return super().__call__(query, key, value, mask)\n",
    "        \n",
    "    class DenseBlock(hk.Module):\n",
    "        \"\"\"A 2-layer MLP\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    init_scale: float,\n",
    "                    widening_factor: int = 4,\n",
    "                    name: Optional[str] = None):\n",
    "            super().__init__(name=name)\n",
    "            self._init_scale = init_scale\n",
    "            self._widening_factor = widening_factor\n",
    "\n",
    "        def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "            hiddens = x.shape[-1]\n",
    "            initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
    "            x = hk.Linear(self._widening_factor * hiddens, w_init=initializer)(x)\n",
    "            x = jax.nn.gelu(x)\n",
    "            return hk.Linear(hiddens, w_init=initializer)(x)\n",
    "        \n",
    "    def layer_norm(x: jnp.ndarray, name: Optional[str] = None) -> jnp.ndarray:\n",
    "        \"\"\"Apply a unique LayerNorm to x with default settings.\"\"\"\n",
    "        return hk.LayerNorm(axis=-1,\n",
    "                            create_scale=True,\n",
    "                            create_offset=True,\n",
    "                            name=name)(x)\n",
    "        \n",
    "    class Transformer(hk.Module):\n",
    "        \"\"\"A transformer stack.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    num_heads: int,\n",
    "                    num_layers: int,\n",
    "                    dropout_rate: float,\n",
    "                    name: Optional[str] = None):\n",
    "            super().__init__(name=name)\n",
    "            self._num_layers = num_layers\n",
    "            self._num_heads = num_heads\n",
    "            self._dropout_rate = dropout_rate\n",
    "\n",
    "        def __call__(self,\n",
    "                    h: jnp.ndarray,\n",
    "                    mask: Optional[jnp.ndarray],\n",
    "                    is_training: bool) -> jnp.ndarray:\n",
    "            \"\"\"Connects the transformer.\n",
    "            Args:\n",
    "            h: Inputs, [B, T, H].\n",
    "            mask: Padding mask, [B, T].\n",
    "            is_training: Whether we're training or not.\n",
    "            Returns:\n",
    "            Array of shape [B, T, H].\n",
    "            \"\"\"\n",
    "\n",
    "            init_scale = 2. / self._num_layers\n",
    "            dropout_rate = self._dropout_rate if is_training else 0.\n",
    "            if mask is not None:\n",
    "                mask = mask[:, None, None, :]\n",
    "\n",
    "            for i in range(self._num_layers):\n",
    "                h_norm = layer_norm(h, name=f'h{i}_ln_1')\n",
    "                h_attn = SelfAttention(\n",
    "                    num_heads=self._num_heads,\n",
    "                    key_size=64,\n",
    "                    w_init_scale=init_scale,\n",
    "                    name=f'h{i}_attn')(h_norm, mask=mask)\n",
    "                h_attn = hk.dropout(hk.next_rng_key(), dropout_rate, h_attn)\n",
    "                h = h + h_attn\n",
    "                h_norm = layer_norm(h, name=f'h{i}_ln_2')\n",
    "                h_dense = DenseBlock(init_scale, name=f'h{i}_mlp')(h_norm)\n",
    "                h_dense = hk.dropout(hk.next_rng_key(), dropout_rate, h_dense)\n",
    "                h = h + h_dense\n",
    "            h = layer_norm(h, name='ln_f')\n",
    "\n",
    "            return h\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    def build_forward_fn(vocab_size: int, d_model: int, num_heads: int,\n",
    "                        num_layers: int, dropout_rate: float):\n",
    "        \"\"\"Create the model's forward pass.\"\"\"\n",
    "        \n",
    "        def embeddings(data: Mapping[str, jnp.ndarray], vocab_size: int) :\n",
    "            tokens = data['obs']\n",
    "            input_mask = jnp.greater(tokens, 0)\n",
    "            seq_length = tokens.shape[1]\n",
    "\n",
    "            # Embed the input tokens and positions.\n",
    "            embed_init = hk.initializers.TruncatedNormal(stddev=0.02)\n",
    "            token_embedding_map = hk.Embed(vocab_size, d_model, w_init=embed_init)\n",
    "            token_embs = token_embedding_map(tokens)\n",
    "            positional_embeddings = hk.get_parameter(\n",
    "                'pos_embs', [seq_length, d_model], init=embed_init)\n",
    "            input_embeddings = token_embs + positional_embeddings\n",
    "            return input_embeddings, input_mask\n",
    "\n",
    "        def forward_fn(data: Mapping[str, jnp.ndarray],\n",
    "                    is_training: bool = True) -> jnp.ndarray:\n",
    "            \"\"\"Forward pass.\"\"\"\n",
    "            input_embeddings, input_mask = embeddings(data, vocab_size)\n",
    "\n",
    "            # Run the transformer over the inputs.\n",
    "            transformer = Transformer(\n",
    "                num_heads=num_heads, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "            output_embeddings = transformer(input_embeddings, input_mask, is_training)\n",
    "\n",
    "            # Reverse the embeddings (untied).\n",
    "            return hk.Linear(vocab_size)(output_embeddings)\n",
    "\n",
    "        return forward_fn\n",
    "    \n",
    "    def lm_loss_fn(forward_fn,\n",
    "               vocab_size: int,\n",
    "               params,\n",
    "               rng,\n",
    "               data: Mapping[str, jnp.ndarray],\n",
    "               is_training: bool = True) -> jnp.ndarray:\n",
    "        \"\"\"Compute the loss on data wrt params.\"\"\"\n",
    "        logits = forward_fn(params, rng, data, is_training)\n",
    "        targets = jax.nn.one_hot(data['target'], vocab_size)\n",
    "        assert logits.shape == targets.shape\n",
    "\n",
    "        mask = jnp.greater(data['obs'], 0)\n",
    "        loss = -jnp.sum(targets * jax.nn.log_softmax(logits), axis=-1)\n",
    "        loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    class GradientUpdater:\n",
    "        \"\"\"A stateless abstraction around an init_fn/update_fn pair.\n",
    "        This extracts some common boilerplate from the training loop.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, net_init, loss_fn,\n",
    "                    optimizer: optax.GradientTransformation):\n",
    "            self._net_init = net_init\n",
    "            self._loss_fn = loss_fn\n",
    "            self._opt = optimizer\n",
    "\n",
    "        @functools.partial(jax.jit, static_argnums=0)\n",
    "        def init(self, master_rng, data):\n",
    "            \"\"\"Initializes state of the updater.\"\"\"\n",
    "            out_rng, init_rng = jax.random.split(master_rng)\n",
    "            params = self._net_init(init_rng, data)\n",
    "            opt_state = self._opt.init(params)\n",
    "            out = dict(\n",
    "                step=np.array(0),\n",
    "                rng=out_rng,\n",
    "                opt_state=opt_state,\n",
    "                params=params,\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        @functools.partial(jax.jit, static_argnums=0)\n",
    "        def update(self, state: Mapping[str, Any], data: Mapping[str, jnp.ndarray]):\n",
    "            \"\"\"Updates the state using some data and returns metrics.\"\"\"\n",
    "            rng, new_rng = jax.random.split(state['rng'])\n",
    "            params = state['params']\n",
    "            loss, g = jax.value_and_grad(self._loss_fn)(params, rng, data)\n",
    "\n",
    "            updates, opt_state = self._opt.update(g, state['opt_state'])\n",
    "            params = optax.apply_updates(params, updates)\n",
    "\n",
    "            new_state = {\n",
    "                'step': state['step'] + 1,\n",
    "                'rng': new_rng,\n",
    "                'opt_state': opt_state,\n",
    "                'params': params,\n",
    "            }\n",
    "\n",
    "            metrics = {\n",
    "                'step': state['step'],\n",
    "                'loss': loss,\n",
    "            }\n",
    "            return new_state, metrics\n",
    "    \n",
    "    def main():\n",
    "        import time\n",
    "        import logging\n",
    "        \n",
    "        # TODO: these needs defining but the tutorial does not specify them\n",
    "        load = ...\n",
    "        batch_size = ...\n",
    "        sequence_length = ...\n",
    "        d_model = ...\n",
    "        num_heads = ...\n",
    "        num_layers = ...\n",
    "        dropout_rate = ...\n",
    "        grad_clip_value = ...\n",
    "        learning_rate = ...\n",
    "        MAX_STEPS = ...\n",
    "        \n",
    "        # Create the dataset.\n",
    "        train_dataset, vocab_size = load(batch_size,\n",
    "                                        sequence_length)\n",
    "        # Set up the model, loss, and updater.\n",
    "        forward_fn = build_forward_fn(vocab_size, d_model, num_heads,\n",
    "                                    num_layers, dropout_rate)\n",
    "        forward_fn = hk.transform(forward_fn)\n",
    "        loss_fn = functools.partial(lm_loss_fn, forward_fn.apply, vocab_size)\n",
    "\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(grad_clip_value),\n",
    "            optax.adam(learning_rate, b1=0.9, b2=0.99))\n",
    "\n",
    "        updater = GradientUpdater(forward_fn.init, loss_fn, optimizer)\n",
    "\n",
    "        # Initialize parameters.\n",
    "        logging.info('Initializing parameters...')\n",
    "        rng = jax.random.PRNGKey(428)\n",
    "        data = next(train_dataset)\n",
    "        state = updater.init(rng, data)\n",
    "\n",
    "        logging.info('Starting train loop...')\n",
    "        prev_time = time.time()\n",
    "        for step in range(MAX_STEPS):\n",
    "            data = next(train_dataset)\n",
    "            state, metrics = updater.update(state, data)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function locally \n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the function for code execution\n",
    "guest_domain_client.api.services.code.request_code_execution(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_domain_client.api.services.code.first_example()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the Data Owner Notebook for Part 2!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Downloading the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_domain_client._api = None\n",
    "_ = guest_domain_client.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = guest_domain_client.api.services.code.haiku_nets_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.get_stderr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySyft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
