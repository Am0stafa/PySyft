{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX 101 - 04 Advanced Automatic Differentiation\n",
    "Link to the original JAX tutorial: https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 - Data Owner Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8,<0.9\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\", reset=True, dev_mode=True)\n",
    "data_owner_client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "data_scientist_client = node.client\n",
    "data_scientist_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "data_scientist_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "# ATTENTION: ALL LIBRARIES USED SHOULD BE DEFINED INSIDE THE FUNCTION CONTEXT!!!\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def higher_order_derivatives():\n",
    "    import jax\n",
    "    f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "\n",
    "    dfdx = jax.grad(f)\n",
    "    d2fdx = jax.grad(dfdx)\n",
    "    d3fdx = jax.grad(d2fdx)\n",
    "    d4fdx = jax.grad(d3fdx)\n",
    "    \n",
    "    print(dfdx(1.))\n",
    "    print(d2fdx(1.))\n",
    "    print(d3fdx(1.))\n",
    "    print(d4fdx(1.))\n",
    "\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def stopping_gradients():\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    \n",
    "    # Value function and initial parameters\n",
    "    value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "    theta = jnp.array([0.1, -0.1, 0.])\n",
    "    \n",
    "    # An example transition.\n",
    "    s_tm1 = jnp.array([1., 2., -1.])\n",
    "    r_t = jnp.array(1.)\n",
    "    s_t = jnp.array([2., 1., 0.])\n",
    "\n",
    "    def td_loss(theta, s_tm1, r_t, s_t):\n",
    "        v_tm1 = value_fn(theta, s_tm1)\n",
    "        target = r_t + value_fn(theta, s_t)\n",
    "        return (target - v_tm1) ** 2\n",
    "\n",
    "    td_update = jax.grad(td_loss)\n",
    "    delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "    print(\"Pseudo naive loss\", delta_theta)\n",
    "    \n",
    "    def td_loss(theta, s_tm1, r_t, s_t):\n",
    "        v_tm1 = value_fn(theta, s_tm1)\n",
    "        target = r_t + value_fn(theta, s_t)\n",
    "        return (jax.lax.stop_gradient(target) - v_tm1) ** 2\n",
    "\n",
    "    td_update = jax.grad(td_loss)\n",
    "    delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "    print(\"Correct loss\", delta_theta)\n",
    "    \n",
    "    perex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n",
    "\n",
    "    # Test it:\n",
    "    batched_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
    "    batched_r_t = jnp.stack([r_t, r_t])\n",
    "    batched_s_t = jnp.stack([s_t, s_t])\n",
    "\n",
    "    print(\"Per example grads\", perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t))\n",
    "\n",
    "    dtdloss_dtheta = jax.grad(td_loss)\n",
    "\n",
    "    print(\"Gradient loss on unbatched inputs\", dtdloss_dtheta(theta, s_tm1, r_t, s_t))\n",
    "\n",
    "    almost_perex_grads = jax.vmap(dtdloss_dtheta)\n",
    "\n",
    "    batched_theta = jnp.stack([theta, theta])\n",
    "    member_gradient = almost_perex_grads(batched_theta, batched_s_tm1, batched_r_t, batched_s_t)\n",
    "    print(\"Gradient for on member of a batch\", member_gradient)\n",
    "\n",
    "    inefficient_perex_grads = jax.vmap(dtdloss_dtheta, in_axes=(None, 0, 0, 0))\n",
    "    grads = inefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)\n",
    "    print(\"Inefficient gradients\", grads)\n",
    "    \n",
    "    perex_grads = jax.jit(inefficient_perex_grads)\n",
    "    grads = perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)\n",
    "    print(\"Efficient gradients\", grads)\n",
    "    \n",
    "    %timeit inefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()\n",
    "    %timeit perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()\n",
    "\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def straight_through_estimator():\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    \n",
    "    def f(x):\n",
    "        return jnp.round(x)  # non-differentiable\n",
    "\n",
    "    def straight_through_f(x):\n",
    "        # Create an exactly-zero expression with Sterbenz lemma that has\n",
    "        # an exactly-one gradient.\n",
    "        zero = x - jax.lax.stop_gradient(x)\n",
    "        return zero + jax.lax.stop_gradient(f(x))\n",
    "\n",
    "    print(\"f(x): \", f(3.2))\n",
    "    print(\"straight_through_f(x):\", straight_through_f(3.2))\n",
    "\n",
    "    print(\"grad(f)(x):\", jax.grad(f)(3.2))\n",
    "    print(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function locally \n",
    "higher_order_derivatives()\n",
    "stopping_gradients()\n",
    "straight_through_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the function for code execution\n",
    "data_scientist_client.api.services.code.request_code_execution(higher_order_derivatives)\n",
    "data_scientist_client.api.services.code.request_code_execution(stopping_gradients)\n",
    "data_scientist_client.api.services.code.request_code_execution(straight_through_estimator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data Owner Reviewing and Approving Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_owner_client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get messages from domain\n",
    "messages = data_owner_client.api.services.messages.get_all()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import review_request, run_submitted_function, accept_request\n",
    "\n",
    "for message in messages:\n",
    "    review_request(message)\n",
    "    real_result = run_submitted_function(message)\n",
    "    accept_request(message, real_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Downloading the Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial complete 👏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_scientist_client.api.services.code.higher_order_derivatives()\n",
    "assert not isinstance(result, sy.SyftError)\n",
    "\n",
    "result = data_scientist_client.api.services.code.stopping_gradients()\n",
    "assert not isinstance(result, sy.SyftError)\n",
    "\n",
    "result = data_scientist_client.api.services.code.straight_through_estimator()\n",
    "assert not isinstance(result, sy.SyftError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if node.node_type.value == \"python\":\n",
    "    node.land()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
